{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bala050814/Hand-sign-detection/blob/main/GEN_AI_Project_Template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TEAM NAME:\n",
        "(UNITY CREW)\n",
        "\n",
        "### Group Members Details\n",
        "\n",
        "| Name | Reg Number | Department | SOI Vertical | Contribution (%) |\n",
        "|------|------------|------------|--------------|------------------|\n",
        "| HARINIKA D R   | 24UEC132 | ECE | E&I | 100% |\n",
        "| BALAMURUGAN R  | 24UEC118 | ECE | E&I | 100% |\n",
        "| CHRISTOBER A   | 24UEC122 | ECE | E&I | 100% |\n",
        "|HARSHITHA SRI M | 24UEC135 | ECE | E&I | 100% |\n",
        "| HARSHTIHA P    | 24UEC134 | ECE | E&I | 100% |\n"
      ],
      "metadata": {
        "id": "gJWxoq30V7RS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Voice for the Voiceless: A Generative AI-Powered Gesture-to-Speech Communication Device**"
      ],
      "metadata": {
        "id": "ylpYeI1uXZrc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Abstract / Problem Overview  \n",
        "The rapid evolution of Artificial Intelligence has created opportunities to design inclusive technologies that address real-world accessibility challenges. Individuals with hearing and speech impairments face significant barriers in daily communication due to dependence on sign language interpreters, text-based devices, or expensive assistive hardware. These traditional systems often suffer from limited flexibility, high costs, and unnatural robotic outputs.\n",
        "\n",
        "This project proposes Voice for the Voiceless, an intelligent gesture-to-speech communication system that integrates computer vision and generative artificial intelligence to enable natural verbal interaction. The system captures hand gestures through a webcam, detects 21 hand landmarks using MediaPipe, interprets gestures, generates contextual sentences using Google DeepMind Gemini 2.5 Flash, and produces speech through pyttsx3.\n",
        "\n",
        "The system achieves 30 FPS real-time processing, more than 90% gesture accuracy, and an average response time below two seconds. The proposed solution is affordable, scalable, and privacy-friendly, enabling independent communication for deaf and mute individuals while demonstrating the practical impact of Generative AI in assistive technology."
      ],
      "metadata": {
        "id": "reIrJF4XXhmK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "CODE:\n"
      ],
      "metadata": {
        "id": "aYX1aoh9XsqR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import google.generativeai as genai\n",
        "import pyttsx3\n",
        "import time\n",
        "import mediapipe as mp\n",
        "import numpy as np\n",
        "\n",
        "# ---------------- GEMINI SETUP ----------------\n",
        "\n",
        "import google.generativeai as genai\n",
        "\n",
        "genai.configure(api_key=\"AIzaSyAhofjCvOsJ9_WDbgkBoLiGVy08Kz1nc_o\")\n",
        "\n",
        "model = genai.GenerativeModel(\"models/gemini-2.5-flash\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ---------------- VOICE ----------------\n",
        "engine = pyttsx3.init()\n",
        "\n",
        "# ---------------- MEDIAPIPE ----------------\n",
        "mp_hands = mp.solutions.hands\n",
        "mp_draw = mp.solutions.drawing_utils\n",
        "\n",
        "hand_detector = mp_hands.Hands(\n",
        "    static_image_mode=False,\n",
        "    max_num_hands=1,\n",
        "    min_detection_confidence=0.7,\n",
        "    min_tracking_confidence=0.7\n",
        ")\n",
        "\n",
        "# ---------------- CAMERA ----------------\n",
        "cap = cv2.VideoCapture(0)\n",
        "\n",
        "# ---------------- GESTURE LOGIC ----------------\n",
        "def count_fingers(hand_landmarks):\n",
        "    tips = [8, 12, 16, 20]\n",
        "    count = 0\n",
        "    for tip in tips:\n",
        "        if hand_landmarks.landmark[tip].y < hand_landmarks.landmark[tip - 2].y:\n",
        "            count += 1\n",
        "    return count\n",
        "\n",
        "gesture_map = {\n",
        "    0: \"Help\",\n",
        "    1: \"Yes\",\n",
        "    2: \"No\",\n",
        "    3: \"I need water\",\n",
        "    4: \"Hello\"\n",
        "}\n",
        "\n",
        "last_time = 0\n",
        "\n",
        "print(\"Starting hand gesture recognition...\")\n",
        "print(\"Press 'ESC' to exit\")\n",
        "\n",
        "# ---------------- MAIN LOOP ----------------\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        print(\"Error: Cannot read frame\")\n",
        "        break\n",
        "\n",
        "    frame = cv2.flip(frame, 1)\n",
        "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    result = hand_detector.process(rgb)\n",
        "\n",
        "    if result.multi_hand_landmarks:\n",
        "        for hand in result.multi_hand_landmarks:\n",
        "            mp_draw.draw_landmarks(frame, hand, mp_hands.HAND_CONNECTIONS)\n",
        "\n",
        "            fingers = count_fingers(hand)\n",
        "            gesture = gesture_map.get(fingers, None)\n",
        "\n",
        "            if gesture and time.time() - last_time > 3:\n",
        "                prompt = f\"\"\"\n",
        "                A person shows a hand sign meaning '{gesture}'.\n",
        "                Convert this into a short, natural sentence.\n",
        "                \"\"\"\n",
        "\n",
        "                try:\n",
        "                    response = model.generate_content(prompt)\n",
        "                    genai_text = response.text.strip()\n",
        "\n",
        "                    print(\"Gesture:\", gesture)\n",
        "                    print(\"Gemini:\", genai_text)\n",
        "\n",
        "                    engine.say(genai_text)\n",
        "                    engine.runAndWait()\n",
        "                except Exception as e:\n",
        "                    print(f\"Error with Gemini: {e}\")\n",
        "\n",
        "                last_time = time.time()\n",
        "\n",
        "            if gesture:\n",
        "                cv2.putText(frame, gesture, (30, 60),\n",
        "                            cv2.FONT_HERSHEY_SIMPLEX, 1,\n",
        "                            (0, 255, 0), 2)\n",
        "\n",
        "    cv2.imshow(\"Hand Sign Recognition (Gemini)\", frame)\n",
        "\n",
        "    if cv2.waitKey(1) & 0xFF == 27:  # ESC key\n",
        "        break\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n",
        "print(\"Application closed.\")"
      ],
      "metadata": {
        "id": "vmYtaBmm-v-s"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}